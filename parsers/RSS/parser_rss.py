import aiohttp
import asyncio
import feedparser
import xml.etree.ElementTree as ET

import datetime

import requests
from bs4 import BeautifulSoup
from dateutil import parser as dateparser
from core.redis_client import redis_client
from parsers.RSS import RSS_URLS
from parsers.RSS.ria_rss import parse_ria
from parsers.RSS.tass_rss import parse_tass
from parsers.text_translator import translate_large_text
from producer import send_raw
from fake_useragent import UserAgent

ua = UserAgent()
headers = {"User-Agent": ua.random}  # —Å–ª—É—á–∞–π–Ω—ã–π User-Agent

async def fetch(session, url: str, use_proxy: int = 0) -> str:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç RSS –ø–æ URL.
    –ï—Å–ª–∏ use_proxy == 1 ‚Üí –ø–æ–¥–∫–ª—é—á–∞–µ–º –ø—Ä–æ–∫—Å–∏.
    """
    kwargs = {"timeout": 15}
    if use_proxy:
        # –ø—Ä–∏–º–µ—Ä: HTTP(S) –ø—Ä–æ–∫—Å–∏
        kwargs["proxy"] = "http://bPLBfY:bN17yH@196.17.67.95:8000"

        # –µ—Å–ª–∏ SOCKS5:
        # kwargs["proxy"] = "socks5://login:password@127.0.0.1:1080"

    async with session.get(url, **kwargs) as r:
        return await r.text()


async def rss_parser():
    print('RSS parser')
    async with aiohttp.ClientSession() as session:
        while True:
            # —Å–æ–∑–¥–∞—ë–º —Å–ø–∏—Å–æ–∫ –∑–∞–¥–∞—á ‚Äî –∫–∞–∂–¥—ã–π RSS –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ
            tasks = [process_rss(session, rss) for rss in RSS_URLS]
            await asyncio.gather(*tasks)

            # –∂–¥—ë–º 5 –º–∏–Ω—É—Ç –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –Ω–æ–≤—ã–π —Ü–∏–∫–ª
            await asyncio.sleep(120)

async def process_rss(session, rss):
    rss_url, rss_id, rss_name, use_proxy = rss

    try:
        # 1. –°–∫–∞—á–∏–≤–∞–µ–º XML
        data = await fetch(session, rss_url, use_proxy=use_proxy)
        feed = feedparser.parse(data)
        root = ET.fromstring(data)

        # 2. –ü–µ—Ä–µ–±–∏—Ä–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ
        for entry in feed.entries:
            url = entry.link
            redis_key = f"seen_urls:{rss_id}"

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å
            added = await redis_client.sadd(redis_key, url)
            if added == 0:
                print(f"‚ö†Ô∏è [{rss_name}] –≤—Å—Ç—Ä–µ—á–µ–Ω–∞ —Å—Ç–∞—Ä–∞—è –Ω–æ–≤–æ—Å—Ç—å ‚Üí –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É")
                break

            # 3. –ö–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            if rss_id in SELECTORS:
                content = get_news_content(url, rss_id)
            elif rss_id in SELECTORS_globalvoices:
                title = await translate_large_text(entry.get("title", ""))
                raw_content = safe_get_text(entry.get("content", entry.get("description", "")))
                content = await translate_large_text(raw_content)
            elif rss_id == 1: content = await parse_ria(url)  # –†–ò–ê –ù–æ–≤–æ—Å—Ç–∏
            elif rss_id == 2: content = await parse_tass(url)  # –¢–ê–°–°
            elif rss_id == 5: # RBC
                ns = {"rbc": "https://www.rbc.ru"}

                guid = entry.get("guid")
                item = None
                for it in root.findall(".//item"):
                    if it.findtext("guid") == guid or it.findtext("link") == url:
                        item = it
                        break

                if item is not None:
                    full = item.find("rbc:full-text", ns)
                    if full is not None and full.text:
                        content = full.text
                    else:
                        content = entry.get("description", "")
            elif rss_id == 6:  # –í–∑–≥–ª—è–¥
                ns = {"yandex": "http://news.yandex.ru"}

                # –∏—â–µ–º item —Å —Ç–∞–∫–∏–º guid –∏–ª–∏ link
                guid = entry.get("guid")
                item = None
                for it in root.findall(".//item"):
                    if it.findtext("guid") == guid or it.findtext("link") == url:
                        item = it
                        break
                if item is not None:
                    full = item.find("yandex:full-text", ns)
                    if full is not None and full.text:
                        content = full.text
                    else:
                        content = entry.get("description", "")

                else:
                    content = entry.get("description", "")
            elif rss_id == 7: #–í–∑–≥–ª—è–¥
                root = ET.fromstring(data)
                ns = {"content": "http://purl.org/rss/1.0/modules/content/"}

                guid = entry.get("guid")
                item = None
                for it in root.findall(".//item"):
                    if it.findtext("guid") == guid or it.findtext("link") == url:
                        item = it
                        break

                if item is not None:
                    full = item.find("content:encoded", ns)
                    if full is not None and full.text:
                        content = full.text
                    else:
                        content = entry.get("description", "")
                else:
                    content = entry.get("description", "")
            else:
                content = entry.get("description", "")
            # 4. –§–æ—Ä–º–∏—Ä—É–µ–º –æ–±—ä–µ–∫—Ç –Ω–æ–≤–æ—Å—Ç–∏
            news = {
                "title": clean_for_telegram(entry.title),
                "description": clean_for_telegram(content),
                "news_date": normalize_date_str(entry),
                "url": url,
                "source": rss_name,
                "flag": "raw",
                "name": "rss_parser"
            }

            # 5. –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ Kafka
            await send_raw(news)
            print(f"üì§ [{rss_name}] –Ω–æ–≤–∞—è –Ω–æ–≤–æ—Å—Ç—å –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞: {entry.title[:60]}")

    except Exception as e:
        print(f"[RSS error] {rss_name} ({rss_url}): {e}")



def normalize_date_str(entry) -> str:
    """
    –ü—Ä–∏–≤–æ–¥–∏—Ç –¥–∞—Ç—É –∏–∑ RSS –∫ —Å—Ç—Ä–æ–∫–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ ISO-8601 (YYYY-MM-DDTHH:MM:SS).
    """
    # 1. –ï—Å–ª–∏ –µ—Å—Ç—å published_parsed (struct_time)
    if entry.get("published_parsed"):
        dt = datetime.datetime(*entry.published_parsed[:6])
        return dt.isoformat()

    # 2. –ï—Å–ª–∏ –µ—Å—Ç—å published (—Å—Ç—Ä–æ–∫–∞)
    if entry.get("published"):
        try:
            dt = dateparser.parse(entry.published)
            return dt.replace(tzinfo=None).isoformat()
        except Exception:
            pass

    # 3. fallback ‚Üí —Ç–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è
    return datetime.datetime.utcnow().isoformat()

import re

def clean_for_telegram(text: str) -> str:
    if not text:
        return ""

    # –∑–∞–º–µ–Ω—è–µ–º <br> –∏ <br/> –Ω–∞ –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏
    text = re.sub(r"<br\s*/?>", "\n", text, flags=re.IGNORECASE)

    # —É–¥–∞–ª—è–µ–º –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ HTML-—Ç–µ–≥–∏ —Ü–µ–ª–∏–∫–æ–º
    text = re.sub(r"<[^>]+>", "", text)

    # —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
    text = re.sub(r"\n\s*\n+", "\n\n", text).strip()

    return text


# üîπ –°–ª–æ–≤–∞—Ä—å —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤
SELECTORS = {
    13: ("div", "news-detail"),
    14: ("div", "field field--name-body field--type-text-with-summary field--label-hidden lightgallery accent-links field__item"),
    15: ("div", "article__body"),
    16: ("div", "single-news__all-text"),
    17: ("div", "NYC0Ro8v fontSize_0"),
    18: ("div", "article_text_wrapper js-search-mark"),

}

SELECTORS_globalvoices = {
    19: ('western-europe'),
    20: ('middle-east-north-africa'),
    21: ('north-america'),
    22: ('latin-america'),
    23: ('eastern-central-europe'),
    24: ('central-asia-caucasus'),
    25: ('east-asia'),
    26: ('war-conflict'),
    27: ('technology'),
    28: ('science'),
    29: ('politics'),
    30: ('health'),
    31: ('development'),
    32: ('breaking-news'),
    33: ('disaster'),
}
def get_news_content(url: str, key: str) -> str:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π.
    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç url –∏ –∫–ª—é—á –¥–ª—è –≤—ã–±–æ—Ä–∞ –Ω—É–∂–Ω–æ–≥–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞.
    """
    try:
        headers = {"User-Agent": ua.random}
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = "utf-8"

        soup = BeautifulSoup(response.text, "lxml")

        if key not in SELECTORS:
            return f"–ö–ª—é—á '{key}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ SELECTORS"

        tag, class_name = SELECTORS[key]
        block = soup.find(tag, class_=class_name)

        if not block:
            return f"–ö–æ–Ω—Ç–µ–Ω—Ç –ø–æ –∫–ª—é—á—É '{key}' –Ω–µ –Ω–∞–π–¥–µ–Ω"

        # –î–ª—è –≤–ª–æ–∂–µ–Ω–Ω–æ–≥–æ div (–∫–∞–∫ –≤ "example")
        inner_div = block.find("div")
        return (inner_div.text if inner_div else block.text).strip()

    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {e}"


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–µ–π
def analyze_news(news_text, word_list):
    for keyword in word_list:
        match = re.search(keyword, news_text, re.IGNORECASE)
        if match:
            return keyword  # –í–µ—Ä–Ω—É—Ç—å –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –∏ —Å–æ–≤–ø–∞–≤—à—É—é —Ñ—Ä–∞–∑—É

    return None


def safe_get_text(value) -> str:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ RSS-–ø–æ–ª–µ–π.
    """
    if isinstance(value, list):
        # feedparser —á–∞—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å "value"
        texts = []
        for v in value:
            if isinstance(v, dict) and "value" in v:
                texts.append(v["value"])
            else:
                texts.append(str(v))
        return " ".join(texts)
    if value is None:
        return ""
    return str(value)
