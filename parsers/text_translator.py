import asyncio
import re
import torch
from bs4 import BeautifulSoup
from transformers import MarianMTModel, MarianTokenizer
import nltk

# –°–∫–∞—á–∞—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (–æ–¥–∏–Ω —Ä–∞–∑)
nltk.download("punkt")

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–≤–æ–¥–∞
model_name = "Helsinki-NLP/opus-mt-en-ru"
tokenizer = MarianTokenizer.from_pretrained(model_name)
model = MarianMTModel.from_pretrained(model_name)

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (GPU –∏–ª–∏ CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

BATCH_SIZE = 16  # –∫–æ–ª-–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –æ–¥–Ω–æ–º –±–∞—Ç—á–µ


def clean_text(text: str) -> str:
    """
    –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç HTML-—Ä–∞–∑–º–µ—Ç–∫–∏ –∏ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤.
    """
    text = BeautifulSoup(text, "lxml").get_text()
    text = re.sub(r"&[a-zA-Z]+;", " ", text)  # –∑–∞–º–µ–Ω—è–µ–º HTML-—Å—É—â–Ω–æ—Å—Ç–∏
    text = re.sub(r"[^\w\s.,!?;:()\-\n]", " ", text, flags=re.UNICODE)  # —É–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã
    text = re.sub(r"\s+", " ", text).strip()  # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã
    return text


async def translate_batch(sentences: list[str]) -> list[str]:
    """
    –ü–µ—Ä–µ–≤–æ–¥ –ø–∞—á–∫–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ.
    """
    def _translate():
        inputs = tokenizer(sentences, return_tensors="pt", padding=True, truncation=True).to(device)
        translated = model.generate(
            **inputs,
            num_beams=2,        # —É—Å–∫–æ—Ä—è–µ–º –∑–∞ —Å—á–µ—Ç —É–º–µ–Ω—å—à–µ–Ω–∏—è beam search
            max_length=512,
            early_stopping=True
        )
        return tokenizer.batch_decode(translated, skip_special_tokens=True)

    return await asyncio.to_thread(_translate)


async def translate_large_text(text: str) -> str:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ –¥–ª–∏–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:
    1) —á–∏—Å—Ç–∏–º —Ç–µ–∫—Å—Ç
    2) —Ä–µ–∂–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    3) –ø–µ—Ä–µ–≤–æ–¥–∏–º –ø–∞—á–∫–∞–º–∏
    """
    clean = clean_text(text)
    sentences = nltk.sent_tokenize(clean, language="english")

    translated_sentences = []
    for i in range(0, len(sentences), BATCH_SIZE):
        batch = sentences[i:i + BATCH_SIZE]
        batch_translations = await translate_batch(batch)
        translated_sentences.extend(batch_translations)

    return " ".join(translated_sentences)


# üîπ –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
async def main():
    raw_text = """
    <p>Hello <b>world</b>! This is a <i>test</i> of the async translation function. üòÄ
    Another sentence with <a href="#">link</a> &amp; some weird chars $$$ !!!</p>
    """
    translated = await translate_large_text(raw_text)
    print(translated)


if __name__ == "__main__":
    asyncio.run(main())
